### **Summary: Gemma with Integral Episodic GNN Memory Layer**

**Note on MVP Scope:** This document describes the overall architectural vision. The initial Minimum Viable Product (MVP) implements a simplified version of this concept. For the MVP, a **Memory Injection Layer** is inserted *between* two of Gemma's standard Transformer blocks. This approach was chosen over modifying an attention head to reduce complexity while still validating the core hypothesis of GNN-based memory integration.

The design integrates a Graph Neural Network (GNN) as an internal, differentiable "memory layer" directly within a Gemma LLM's Transformer stack. This creates a Unified Latent Space (ULS) where Gemma's textual understanding and the GNN's episodic memory are intrinsically fused.

1.  **Unified Latent Space (ULS):** Gemma's internal hidden state space serves as the ULS, where all computations (text processing, memory retrieval, and integration) occur.
2.  **MVP: Memory Injection Layer (GNN Integration Point):**
    *   A custom `MemoryInjectionLayer` is inserted between two of Gemma's Transformer blocks (e.g., after layer 8). This layer contains the GNN.
    *   **Query Generation (from Gemma):** The layer takes the sequence of hidden states from the preceding Gemma layer. It pools these states (e.g., via mean pooling) to create a single `query_vector` that represents the current context.
    *   **Memory Retrieval (from GNN):**
        *   This `query_vector` is passed to the internal GNN.
        *   The GNN performs a differentiable lookup/aggregation over its graph of memory nodes.
        *   The GNN outputs a single `episodic_context_embedding` that represents the relevant memory.
    *   **Integration:** The `episodic_context_embedding` is projected to match the dimension of Gemma's hidden states. This projected vector is then added back to the original input hidden states. The resulting memory-enriched hidden states are then passed to the next Gemma layer.
3.  **Dynamic GNN Graph (Episodic Memory Store):**
    *   The GNN maintains an external but dynamically updated graph G.
    *   **Nodes:** Each node represents a past "episode" (e.g., a sentence of context). Its feature vector is an embedding generated by the model's own frozen `embed_tokens` layer.
    *   **Edges:** Primarily temporal (connecting E_t to E_{t-1}), capturing conversational flow.
    *   **Learning:** The GNN's internal parameters and the `MemoryInjectionLayer`'s projection parameters are trained. The base Gemma model remains frozen.
4.  **End-to-End Fine-tuning (for MVP Components):** The integrated system is fine-tuned on a synthetic dataset.
    *   **Training:** The data loader's `collate_fn` is responsible for accessing the model's embedding logic to pre-build the GNN memory graph for each training sample. Gradients flow only through the GNN and injection layer.
    *   **Inference:** During live interaction, the model uses the same internal embedding logic to dynamically add new context sentences as nodes to its memory graph in real-time.

This design makes the GNN an organic extension of Gemma's reasoning process, operating as a core processing step within its forward pass rather than an external API.