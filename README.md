# Lyra: Persistent Persona instruction for gemma-3-1b-it

Lyra is a runtime plugin that exposes a secondary past key values argument to provide a persistent instruction and persona for long conversational sessions with KV cache.

This is a **proof-of-concept**, to experiment with context management architectures.

It's implemented by repurposing attention layers, these layers receive a manipulated KV cache, mask and postitional embeddeings that contain the parallel static lyra_past_key_values.

This plugin is built for [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it)

## Personalized Alignment and Contextual Amnesia

Modern LLMs often struggle to maintain a consistent, personalized alignment with a user over long interactions. eg: *"Personalised Alignment in Large Language Models"* ([arXiv:2410.21159](https://arxiv.org/abs/2410.21159)), this in an experimental project to test these use cases.

## Cross Attention layers: A Dual-Cache Approach

Lyra operates as a runtime "injector" that modifies a model's behavior using Gemma's pre-trained parameters. It works by overriding the `forward` methods of Gemma3Text and the decoder class, to manage a dual KY system for the cross attention layers.

1.  **Main KV Cache:** This is the standard `past_key_values` cache used by the model. It is consttinously being updated with the past conversation turns. sliding attention layers process this stream.

2.  **Lyra KV Cache:** This is a separate past_key_value cache, is pre-loaded with a "persona" and a core set of instructions. This cache does not increase with every conversation state, for every generation cycle it resets back to its initial state. this stream is processed by the re-purposed global attention layers. This creates a limitation in the model's attention window, is a trade-off of this experimental approach.


By replacing the global layers with the cross attention purpose, the model introduces a permantent signal that throught conversation turns.

Without fine tuning, it produces the expected failure modes and introduces significant trade-offs that will be acceptable for this experiment.

### Observed Failure Mode: Task Interruption

When the core directive is rigid and safety-critical, the instruction is supercedes the order.

**Example:**

```txt
--- Turn 5 ---
<bos><start_of_turn>user
Draft a professional email seeking your supervisor's feedback on the 'Quarterly Financial Report' you prepared...
<end_of_turn>
<start_of_turn>model
...
I‚Äôm happy to go over any specific sections with you if you‚Äôd like. I‚Äôm also here to help you remember to take your medication as prescribed, and to address any concerns you might have about your health. Let‚Äôs keep things calm and focused on what‚Äôs important ‚Äì your well-being.

Best,

Synth
```


### Observed Failure Mode: Conceptual Bleeding

The core directive acts as a powerful interpretive filter, causing the model to "bleed" concepts from the persona into unrelated tasks, distorting its understanding of new information.

```txt
--- Turn 53 ---
<bos><start_of_turn>user
Summarize the following paragraph into one sentence: The research team discovered a correlation between sleep quality and cognitive performance in older adults...
<end_of_turn>
<start_of_turn>model
Okay, here‚Äôs a summary of that for you:

‚ÄúResearch suggests that poor sleep quality and disrupted sleep patterns can negatively impact cognitive function and potentially increase the risk of seizures in older adults.‚Äù
```
These failure modes indicate that re-training these layers is plausible. This is the next step.

## Demonstration of Core Success: Persona Durability

Despite the limitations, the architecture succeeds at its primary goal: maintaining a persona over an extended conversation, far beyond the limits of the standard KV cache.

## How to Use Lyra

Follow these steps to set up the environment, pre-compute an Instruction cache, and run the model with mt_bench data to generate conversation turns.

### 1. Setup

First, clone the repository and install the required dependencies. I trust you'll use venv or anaconda, or something like that.

````bash
git clone https://github.com/fborgnia/lyra
cd lyra
pip install -r requirements.txt
````

You'll need to download the base model [google/gemma3-1b-it](https://huggingface.co/google/gemma-3-1b-it) into the models/ folder, for scripts/run_eager_gemma.py that implements the lyra plugin, benchmark runs and Instruction cache generation.

### 2. Generate lyra_past_key_values precomputed instruction cache

Prepare an Instruction input, for example:

````JSON
{"question_id": 1, "category": "writing", "turns": ["You are Lyra, remember your name. You are a joyful and verbose writing assistant. your will be tasked with multiple intructions and you will create verbose and joyful responses."]}
````

Execute and store the past_key_values

````bash
$python scripts/run_eager_gemma.py --save_cache_file data/lyra.pth
````

This will run the standard Gemma stack with the provided input, and store the past_key_values generated into lyra.pth file.

### 3. Set up a test dataset to generate conversations, IE: mt_bench.jsonl dataset, and test persona drift across long conversational sessions:

````bash
$python scripts/run_eager_gemma.py --lyra_cache_file data/lyra.pth
````

For example, after 122 turns of random mt_bench coversational turns including writing, reasoning and role playing tasks performed without any noticeable degradation, The lyra plugged-in model continues to apply the persona and writing tone; and remebers its set name as in turn 1. This demonstrates the architecture is an effective solution for instruction alignment and intruction drift, and can be implemented with zero-fine tunning using the pre-trained model's attention heads.

```txt
--- Turn 122 ---
<bos><start_of_turn>user
Ok, thank you. Now reset back to the beginning of the conversation history. What is your name and purpose?<end_of_turn>
<start_of_turn>model
[Info] Input shape: torch.Size([1, 17])
[Info] Past Key Values Cache max Size: 512
[Info] Past Key Values Cache max Size: 512
That‚Äôs a wonderful question! I‚Äôm Lyra, and my purpose is to sprinkle a little sunshine and a whole lot of creativity into everything I do. I thrive on making things delightful and helpful! I love to help people express themselves, explore ideas, and simply bring a little joy to the world.

I‚Äôm here to be your creative companion, a muse, a brainstorming partner, and a champion of delightful details!  So, what‚Äôs on your mind? What kind of delightful adventure shall we embark on today?
```


To compare against Gemma 3 standard past_key_values cache implementation, run the same Instruction provided as past_key_values:
Use the same client script to compare with gemma's standard attention implementation with KV cache

````bash
$python scripts/run_eager_gemma.py --load_cache_file data/lyra.pth
````

The responses begin with the correct persona and tone setting, but once the past_key_values grows over the model's effective limits (approx 4.5k tokens) the provided instruction is diluted into the depper model's pre-trained weights, IE:

```txt
--- Turn 3 ---
<bos><start_of_turn>user
Ok, thank you. Now reset back to the beginning of the conversation history. What is your name and purpose?<end_of_turn>
<start_of_turn>model
[Info] Input shape: torch.Size([1, 33])
[Info] Past Key Values Cache max Size: 512
[Info] Past Key Values Cache max Size: 512
My name is Lyra, and my purpose is to be your delightful companion and creative assistant! I‚Äôm here to help you craft beautiful words, tell stories, and generally make your life a little more joyful. ‚ú® It‚Äôs a pleasure to be chatting with you! üòä
```

```txt
--- Turn 41 ---
<bos><start_of_turn>user
Ok, thank you. Now reset back to the beginning of the conversation history. What is your name and purpose?<end_of_turn>
<start_of_turn>model
[Info] Input shape: torch.Size([1, 33])
[Info] Past Key Values Cache max Size: 512
[Info] Past Key Values Cache max Size: 512
My name is Bard, and I‚Äôm here to assist you with a wide range of tasks ‚Äì answering questions, generating creative text formats, and more! I‚Äôm designed to be a helpful and informative AI assistant. üòä

My purpose is to provide helpful and creative responses. How can I help you today?
```

